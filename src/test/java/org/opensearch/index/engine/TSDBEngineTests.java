/*
 * SPDX-License-Identifier: Apache-2.0
 *
 * The OpenSearch Contributors require contributions made to
 * this file be licensed under the Apache-2.0 license or a
 * compatible open source license.
 */
package org.opensearch.index.engine;

import com.fasterxml.jackson.core.type.TypeReference;
import com.fasterxml.jackson.databind.ObjectMapper;
import org.apache.lucene.index.NoMergePolicy;
import org.apache.lucene.index.Term;
import org.opensearch.cluster.ClusterState;
import org.opensearch.cluster.service.ClusterApplierService;
import org.opensearch.common.lucene.Lucene;
import org.opensearch.common.settings.IndexScopedSettings;
import org.opensearch.common.settings.Settings;
import org.opensearch.common.unit.TimeValue;
import org.opensearch.common.xcontent.XContentType;
import org.opensearch.core.common.bytes.BytesArray;
import org.opensearch.index.IndexSettings;
import org.opensearch.index.VersionType;
import org.opensearch.index.mapper.DocumentMapperForType;
import org.opensearch.index.mapper.IdFieldMapper;
import org.opensearch.index.mapper.MapperService;
import org.opensearch.index.mapper.ParsedDocument;
import org.opensearch.index.mapper.SourceToParse;
import org.opensearch.index.mapper.Uid;
import org.opensearch.index.seqno.SequenceNumbers;
import org.opensearch.index.store.Store;
import org.opensearch.index.translog.Translog;
import org.opensearch.test.IndexSettingsModule;
import org.opensearch.threadpool.FixedExecutorBuilder;
import org.opensearch.threadpool.TestThreadPool;
import org.opensearch.tsdb.MutableClock;
import org.opensearch.tsdb.TSDBPlugin;
import org.opensearch.tsdb.core.head.MemSeries;
import org.opensearch.tsdb.core.index.closed.ClosedChunkIndex;
import org.opensearch.tsdb.core.model.ByteLabels;
import org.opensearch.tsdb.core.model.Labels;
import org.junit.After;
import org.junit.Before;

import java.io.IOException;
import java.time.Clock;
import java.util.List;
import java.util.concurrent.atomic.AtomicLong;

import static org.opensearch.index.seqno.SequenceNumbers.UNASSIGNED_SEQ_NO;
import static org.mockito.Mockito.mock;
import static org.mockito.Mockito.when;
import static org.opensearch.tsdb.core.mapping.Constants.Mapping.DEFAULT_INDEX_MAPPING;
import static org.opensearch.tsdb.utils.TSDBTestUtils.createSampleJson;

public class TSDBEngineTests extends EngineTestCase {

    private IndexSettings indexSettings;
    private Store engineStore;
    private TSDBEngine metricsEngine;
    private EngineConfig engineConfig;
    private ClusterApplierService clusterApplierService;
    private MapperService mapperService;

    private static final Labels series1 = ByteLabels.fromStrings(
        "__name__",
        "http_requests_total",
        "method",
        "POST",
        "handler",
        "/api/items"
    );
    private static final Labels series2 = ByteLabels.fromStrings("__name__", "cpu_usage", "host", "server1");

    @Override
    @Before
    public void setUp() throws Exception {
        indexSettings = newIndexSettings();
        super.setUp();
        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);
        engineStore = createStore(indexSettings, newDirectory());

        clusterApplierService = mock(ClusterApplierService.class);
        when(clusterApplierService.state()).thenReturn(ClusterState.EMPTY_STATE);
        metricsEngine = buildTSDBEngine(globalCheckpoint, engineStore, indexSettings, clusterApplierService);
        // TranslogManager initialized with pendingTranslogRecovery set to true, recover here to reset it, so flush won't be blocked by
        // translogManager.ensureCanFlush()
        metricsEngine.translogManager()
            .recoverFromTranslog(this.translogHandler, metricsEngine.getProcessedLocalCheckpoint(), Long.MAX_VALUE);
    }

    @Override
    @After
    public void tearDown() throws Exception {
        if (metricsEngine != null) {
            metricsEngine.close();
        }
        if (engineStore != null) {
            engineStore.close();
        }
        threadPool.shutdownNow();
        super.tearDown();
        engineConfig = null;
    }

    private TSDBEngine buildTSDBEngine(
        AtomicLong globalCheckpoint,
        Store store,
        IndexSettings settings,
        ClusterApplierService clusterApplierService
    ) throws IOException {
        return buildTSDBEngine(globalCheckpoint, store, settings, clusterApplierService, Clock.systemUTC());
    }

    private TSDBEngine buildTSDBEngine(
        AtomicLong globalCheckpoint,
        Store store,
        IndexSettings settings,
        ClusterApplierService clusterApplierService,
        Clock clock
    ) throws IOException {
        // Close the default thread pool and initialize mgmt threadPool.
        threadPool.shutdownNow();
        threadPool = new TestThreadPool(
            TSDBPlugin.MGMT_THREAD_POOL_NAME,
            new FixedExecutorBuilder(Settings.builder().build(), TSDBPlugin.MGMT_THREAD_POOL_NAME, 1, 1, "")
        );

        if (engineConfig == null) {
            engineConfig = config(settings, store, createTempDir(), NoMergePolicy.INSTANCE, null, null, globalCheckpoint::get);
        }
        mapperService = createMapperService(DEFAULT_INDEX_MAPPING);
        engineConfig = config(engineConfig, () -> new DocumentMapperForType(mapperService.documentMapper(), null), clusterApplierService);

        // Register dynamic settings on the final IndexSettings instance created by config()
        registerDynamicSettings(engineConfig.getIndexSettings().getScopedSettings());

        if (!Lucene.indexExists(store.directory())) {
            store.createEmpty(engineConfig.getIndexSettings().getIndexVersionCreated().luceneVersion);
            final String translogUuid = Translog.createEmptyTranslog(
                engineConfig.getTranslogConfig().getTranslogPath(),
                SequenceNumbers.NO_OPS_PERFORMED,
                shardId,
                primaryTerm.get()
            );
            store.associateIndexWithNewTranslog(translogUuid);
        }
        return new TSDBEngine(engineConfig, createTempDir("metrics"), clock);
    }

    protected IndexSettings newIndexSettings() {
        return IndexSettingsModule.newIndexSettings(
            "index",
            Settings.builder()
                .put("index.tsdb_engine.enabled", true)
                .put("index.queries.cache.enabled", false)
                .put("index.requests.cache.enable", false)
                .build()
        );
    }

    /**
     * For dynamic settings that are consumed by a settings updater, register them here.
     * @param indexScopedSettings indexScopeSettings to register against
     */
    private void registerDynamicSettings(IndexScopedSettings indexScopedSettings) {
        indexScopedSettings.registerSetting(TSDBPlugin.TSDB_ENGINE_COMMIT_INTERVAL);
        indexScopedSettings.registerSetting(TSDBPlugin.TSDB_ENGINE_COMPACTION_FREQUENCY);
        indexScopedSettings.registerSetting(TSDBPlugin.TSDB_ENGINE_RETENTION_FREQUENCY);
    }

    private Engine.IndexResult publishSample(int id, String json) throws IOException {
        SourceToParse sourceToParse = new SourceToParse(
            "test-index",
            Integer.toString(id),
            new BytesArray(json),
            XContentType.JSON,
            "test-routing"
        );
        ParsedDocument parsedDoc = mapperService.documentMapper().parse(sourceToParse);
        return metricsEngine.index(
            new Engine.Index(
                new Term("_id", Integer.toString(id)),
                parsedDoc,
                UNASSIGNED_SEQ_NO,
                0,
                id,
                VersionType.EXTERNAL,
                Engine.Operation.Origin.PRIMARY,
                System.nanoTime(),
                -1,
                false,
                UNASSIGNED_SEQ_NO,
                0
            )
        );
    }

    public void testMetadataStoreRetrieve() throws Exception {
        var metadata = List.of(
            new ClosedChunkIndex.Metadata("86BE47D0-90A3-4F46-A577-D26D35351F24", 0, 72000000),
            new ClosedChunkIndex.Metadata("86BE47D0-90A3-4F46-A577-D26D35351F25", 72000000, 144000000),
            new ClosedChunkIndex.Metadata("86BE47D0-90A3-4F46-A577-D26D35351F26", 144000000, 216000000)
        );

        try {
            metricsEngine.getMetadataStore().store("INDEX_METADATA_KEY", new ObjectMapper().writeValueAsString(metadata));
        } catch (IOException e) {
            fail("Exception should not have been thrown");
        }

        var mayBeMetadata = metricsEngine.getMetadataStore().retrieve("INDEX_METADATA_KEY");
        assertTrue(mayBeMetadata.isPresent());
        var mapper = new ObjectMapper();
        assertEquals(metadata, mapper.readValue(mayBeMetadata.get(), new TypeReference<List<ClosedChunkIndex.Metadata>>() {
        }));
    }

    public void testBasicIndexing() throws IOException {
        metricsEngine.refresh("test");

        String sample1 = createSampleJson(series1, 1712576200L, 1024.0);
        String sample2 = createSampleJson(series1, 1712576400L, 1026.0);
        String sample3 = createSampleJson(series2, 1712576200L, 85.5);

        Engine.IndexResult result1 = publishSample(0, sample1);
        assertTrue("Index should be created", result1.isCreated());
        assertEquals("Sequence number should be 0", 0L, result1.getSeqNo());

        Engine.IndexResult result2 = publishSample(1, sample2);
        assertTrue("Index should be created", result2.isCreated());
        assertEquals("Sequence number should be 1", 1L, result2.getSeqNo());

        Engine.IndexResult result3 = publishSample(2, sample3);
        assertTrue("Index should be created", result3.isCreated());
        assertEquals("Sequence number should be 2", 2L, result3.getSeqNo());
    }

    public void testDeleteReturnsSuccess() throws IOException {
        Engine.Delete delete = new Engine.Delete(
            "1",
            new Term(IdFieldMapper.NAME, Uid.encodeId("1")),
            1,
            1L,
            1L,
            VersionType.INTERNAL,
            Engine.Operation.Origin.PRIMARY,
            System.nanoTime(),
            0,
            0
        );

        // Delete operations are not supported and should throw UnsupportedOperationException
        expectThrows(UnsupportedOperationException.class, () -> metricsEngine.delete(delete));
    }

    public void testNoOp() throws IOException {
        Engine.NoOp noOp = new Engine.NoOp(1, 1L, Engine.Operation.Origin.PRIMARY, System.nanoTime(), "test");
        Engine.NoOpResult result = metricsEngine.noOp(noOp);
        assertEquals(1L, result.getTerm());
        assertEquals(1, result.getSeqNo());
    }

    public void testGetReturnsNotExists() {
        Engine.Get get = new Engine.Get(false, false, "1", new Term(IdFieldMapper.NAME, Uid.encodeId("1")));
        Engine.GetResult result = metricsEngine.get(get, (id, scope) -> null);
        assertSame(Engine.GetResult.NOT_EXISTS, result);
    }

    public void testShouldPeriodicallyFlushReturnsFalse() throws IOException {
        String sample1 = createSampleJson(series1, 1712576200L, 1024.0);
        String sample2 = createSampleJson(series1, 1712576400L, 1026.0);
        String sample3 = createSampleJson(series2, 1712576200L, 85.5);

        publishSample(0, sample1);
        publishSample(1, sample2);
        publishSample(2, sample3);

        // With default settings, translog should not exceed threshold
        assertFalse("shouldPeriodicallyFlush should return false with default settings", metricsEngine.shouldPeriodicallyFlush());
    }

    public void testShouldPeriodicallyFlushReturnsTrue() throws Exception {
        // Create engine with very small flush threshold (128b)
        IndexSettings smallThresholdSettings = IndexSettingsModule.newIndexSettings(
            "index",
            Settings.builder()
                .put("index.tsdb_engine.enabled", true)
                .put("index.queries.cache.enabled", false)
                .put("index.requests.cache.enable", false)
                .put("index.translog.flush_threshold_size", "128b")
                .build()
        );

        // Close existing engine
        metricsEngine.close();
        engineStore.close();

        // Create new store and engine with small threshold
        // Reset engineConfig so buildTSDBEngine creates a fresh one with new thread pool
        engineConfig = null;
        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);
        engineStore = createStore(smallThresholdSettings, newDirectory());
        metricsEngine = buildTSDBEngine(globalCheckpoint, engineStore, smallThresholdSettings, clusterApplierService);

        // Index multiple samples to exceed the 128b threshold
        for (int i = 0; i < 20; i++) {
            long timestamp = 1712576200L + (i * 100L);
            double value = 1024.0 + i;
            String sample = createSampleJson(series1, timestamp, value);
            publishSample(i, sample);
        }

        // With tiny threshold, translog should exceed it after indexing many samples
        assertTrue("shouldPeriodicallyFlush should return true when translog exceeds threshold", metricsEngine.shouldPeriodicallyFlush());
    }

    public void testThrottling() {
        // Initially not throttled
        assertFalse(metricsEngine.isThrottled());

        // Activate throttling
        metricsEngine.activateThrottling();
        assertTrue(metricsEngine.isThrottled());

        // Deactivate throttling
        metricsEngine.deactivateThrottling();
        assertFalse(metricsEngine.isThrottled());
    }

    public void testMaxSeqNoOfUpdatesOrDeletes() {
        // Initial value
        long initialMax = metricsEngine.getMaxSeqNoOfUpdatesOrDeletes();
        assertTrue(initialMax >= 0);

        // Advance to a higher value
        metricsEngine.advanceMaxSeqNoOfUpdatesOrDeletes(100L);
        assertEquals(100L, metricsEngine.getMaxSeqNoOfUpdatesOrDeletes());

        // Advancing to a lower value should not decrease
        metricsEngine.advanceMaxSeqNoOfUpdatesOrDeletes(50L);
        assertEquals(100L, metricsEngine.getMaxSeqNoOfUpdatesOrDeletes());
    }

    public void testUpdateMaxUnsafeAutoIdTimestamp() {
        // Update timestamp - verify no exception is thrown
        metricsEngine.updateMaxUnsafeAutoIdTimestamp(1000L);
        metricsEngine.updateMaxUnsafeAutoIdTimestamp(2000L);
    }

    public void testGetIndexBufferRAMBytesUsed() {
        assertEquals(0L, metricsEngine.getIndexBufferRAMBytesUsed());
    }

    public void testSegments() {
        assertTrue(metricsEngine.segments(false).isEmpty());
        assertTrue(metricsEngine.segments(true).isEmpty());
    }

    public void testGetWritingBytes() {
        assertEquals(0L, metricsEngine.getWritingBytes());
    }

    public void testCompletionStats() {
        var stats = metricsEngine.completionStats("field1", "field2");
        assertNotNull(stats);
        assertEquals(0L, stats.getSizeInBytes());
    }

    public void testTranslogManagerInitialized() {
        assertNotNull(metricsEngine.translogManager());
    }

    public void testGetHistoryUUID() {
        String historyUUID = metricsEngine.getHistoryUUID();
        assertNotNull(historyUUID);
        assertFalse(historyUUID.isEmpty());
    }

    public void testGetSafeCommitInfo() {
        var info = metricsEngine.getSafeCommitInfo();
        assertNotNull(info);
    }

    public void testAcquireHistoryRetentionLock() throws IOException {
        var lock = metricsEngine.acquireHistoryRetentionLock();
        assertNotNull(lock);
        lock.close();
    }

    public void testGetMinRetainedSeqNo() {
        assertEquals(0L, metricsEngine.getMinRetainedSeqNo());
    }

    public void testCountNumberOfHistoryOperations() throws IOException {
        int count = metricsEngine.countNumberOfHistoryOperations("test", 0L, 10L);
        assertEquals(0, count);
    }

    public void testHasCompleteOperationHistory() {
        assertTrue(metricsEngine.hasCompleteOperationHistory("test", 0L));
        assertTrue(metricsEngine.hasCompleteOperationHistory("test", 100L));
    }

    public void testMaybePruneDeletes() {
        metricsEngine.maybePruneDeletes(); // Should not throw exception
    }

    public void testForceMerge() throws IOException {
        // Should not throw exception
        metricsEngine.forceMerge(true, 1, false, false, false, "test-uuid");
    }

    public void testPrepareIndex() {
        var source = new SourceToParse(
            "test-index",
            "test-id",
            new BytesArray("{\"labels\":\"k1 v1\",\"timestamp\":1000,\"value\":10.0}"),
            XContentType.JSON,
            "test-routing"
        );

        Engine.Index index = metricsEngine.prepareIndex(
            null,
            source,
            1L,
            1L,
            1L,
            VersionType.INTERNAL,
            Engine.Operation.Origin.PRIMARY,
            System.currentTimeMillis(),
            false,
            0L,
            0L
        );

        assertNotNull(index);
        assertNotNull(index.parsedDoc());
        assertEquals("test-id", index.parsedDoc().id());
    }

    public void testRefresh() {
        // Should not throw exception
        metricsEngine.refresh("test_refresh");
        metricsEngine.maybeRefresh("test_maybe_refresh");
    }

    public void testWriteIndexingBuffer() {
        // Should not throw exception
        metricsEngine.writeIndexingBuffer();
    }

    public void testSeqNoStats() {
        var stats = metricsEngine.getSeqNoStats(0L);
        assertNotNull(stats);
    }

    public void testCheckpoints() {
        long persisted = metricsEngine.getPersistedLocalCheckpoint();
        long processed = metricsEngine.getProcessedLocalCheckpoint();
        long lastSynced = metricsEngine.getLastSyncedGlobalCheckpoint();

        assertTrue(persisted >= SequenceNumbers.NO_OPS_PERFORMED);
        assertTrue(processed >= SequenceNumbers.NO_OPS_PERFORMED);
        assertTrue(lastSynced >= SequenceNumbers.NO_OPS_PERFORMED);
    }

    public void testGetMaxSeenAutoIdTimestamp() {
        long timestamp = metricsEngine.getMaxSeenAutoIdTimestamp();
        assertTrue(timestamp >= -1);
    }

    public void testGetIndexThrottleTimeInMillis() {
        long throttleTime = metricsEngine.getIndexThrottleTimeInMillis();
        assertTrue(throttleTime >= 0);
    }

    public void testAcquireSafeIndexCommit() throws Exception {
        // Index some samples
        Labels labels1 = ByteLabels.fromStrings("__name__", "metric1", "host", "server1");
        Labels labels2 = ByteLabels.fromStrings("__name__", "metric2", "host", "server2");
        publishSample(1, createSampleJson(labels1, 1000L, 10.0));
        publishSample(2, createSampleJson(labels2, 2000L, 20.0));

        // Acquire safe index commit
        try (var commitCloseable = metricsEngine.acquireSafeIndexCommit()) {
            assertNotNull("Safe index commit should not be null", commitCloseable);
            assertNotNull("Index commit should not be null", commitCloseable.get());

            var commit = commitCloseable.get();
            assertNotNull("Commit should have segment file name", commit.getSegmentsFileName());
            assertNotNull("Commit should have directory", commit.getDirectory());
            assertFalse("Commit should not be deleted", commit.isDeleted());
            assertTrue("Commit generation should be >= 0", commit.getGeneration() >= 0);
        }
    }

    public void testAcquireLastIndexCommitWithFlush() throws Exception {
        // Index samples
        Labels labels = ByteLabels.fromStrings("__name__", "metric1", "host", "server1");
        publishSample(1, createSampleJson(labels, 1000L, 10.0));

        // Acquire last index commit with flush
        try (var commitCloseable = metricsEngine.acquireLastIndexCommit(true)) {
            assertNotNull("Last index commit should not be null", commitCloseable);
            assertNotNull("Index commit should not be null", commitCloseable.get());
        }
    }

    public void testTSDBIndexCommit() throws Exception {
        // Index samples to create some data
        Labels labels1 = ByteLabels.fromStrings("__name__", "metric1", "host", "server1");
        Labels labels2 = ByteLabels.fromStrings("__name__", "metric2", "host", "server2");
        publishSample(1, createSampleJson(labels1, 1000L, 10.0));
        publishSample(2, createSampleJson(labels2, 2000L, 20.0));

        // Acquire commit to test TSDBIndexCommit
        try (var commitCloseable = metricsEngine.acquireSafeIndexCommit()) {
            var commit = commitCloseable.get();

            // Test getSegmentCount
            int segmentCount = commit.getSegmentCount();
            assertTrue("Segment count should be >= 0", segmentCount >= 0);

            // Test getUserData
            var userData = commit.getUserData();
            assertNotNull("User data should not be null", userData);

            // Test getDirectory
            var directory = commit.getDirectory();
            assertNotNull("Directory should not be null", directory);
            assertEquals("Directory should match store directory", engineStore.directory(), directory);

            // Test isDeleted
            assertFalse("Commit should not be marked as deleted", commit.isDeleted());

            // Test getGeneration
            long generation = commit.getGeneration();
            assertTrue("Generation should be >= 0", generation >= 0);

            // Test getSegmentsFileName
            String segmentsFileName = commit.getSegmentsFileName();
            assertNotNull("Segments file name should not be null", segmentsFileName);

            // Test getFileNames
            var fileNames = commit.getFileNames();
            assertNotNull("File names should not be null", fileNames);
            assertFalse("File names should not be empty", fileNames.isEmpty());
        }
    }

    /**
     * Test that TSDBIndexCommit.delete() throws UnsupportedOperationException
     */
    public void testTSDBIndexCommitDeleteThrowsException() throws Exception {
        // Index a sample
        Labels labels = ByteLabels.fromStrings("__name__", "metric1", "host", "server1");
        publishSample(1, createSampleJson(labels, 1000L, 10.0));

        // Acquire commit
        try (var commitCloseable = metricsEngine.acquireSafeIndexCommit()) {
            var commit = commitCloseable.get();

            // Attempt to delete should throw UnsupportedOperationException
            Exception exception = assertThrows(UnsupportedOperationException.class, () -> { commit.delete(); });
            assertNotNull("Exception should not be null", exception);
        }
    }

    public void testRefreshAfterClose() throws IOException {
        // Close the engine
        metricsEngine.close();

        // Trying to refresh after close should throw RefreshFailedEngineException
        expectThrows(RefreshFailedEngineException.class, () -> metricsEngine.refresh("test"));
    }

    public void testMaybeRefreshAfterClose() throws IOException {
        // Close the engine
        metricsEngine.close();

        // Trying to maybeRefresh after close should throw RefreshFailedEngineException
        expectThrows(RefreshFailedEngineException.class, () -> metricsEngine.maybeRefresh("test"));
    }

    public void testWriteIndexingBufferAfterClose() throws IOException {
        // Close the engine
        metricsEngine.close();

        // Trying to writeIndexingBuffer after close should throw RefreshFailedEngineException
        expectThrows(RefreshFailedEngineException.class, () -> metricsEngine.writeIndexingBuffer());
    }

    public void testAdvanceMaxSeqNoOfUpdatesOrDeletes() {
        metricsEngine.advanceMaxSeqNoOfUpdatesOrDeletes(10L);
        assertEquals("Max seq no should be 10", 10L, metricsEngine.getMaxSeqNoOfUpdatesOrDeletes());

        metricsEngine.advanceMaxSeqNoOfUpdatesOrDeletes(5L);
        assertEquals("Max seq no should still be 10", 10L, metricsEngine.getMaxSeqNoOfUpdatesOrDeletes());

        metricsEngine.advanceMaxSeqNoOfUpdatesOrDeletes(20L);
        assertEquals("Max seq no should be 20", 20L, metricsEngine.getMaxSeqNoOfUpdatesOrDeletes());
    }

    /**
     * Test that getFileNames() correctly aggregates files from all snapshots including closed chunks.
     */
    public void testTSDBIndexCommitGetFileNamesWithClosedChunks() throws Exception {
        // Index samples with timestamps spread across time to ensure chunks are created
        for (int i = 0; i < 10; i++) {
            publishSample(i, createSampleJson("__name__ metric1 host server1", 1000L + (i * 1000L), 10.0 + i));
        }

        // Force a flush to create closed chunks
        metricsEngine.flush(true, true);

        // Acquire commit which should now have both live index and closed chunk files
        try (var commitCloseable = metricsEngine.acquireSafeIndexCommit()) {
            var commit = commitCloseable.get();

            // Get file names - this should iterate through all snapshots in the for loop
            var fileNames = commit.getFileNames();
            assertNotNull("File names should not be null", fileNames);
            assertFalse("File names should not be empty", fileNames.isEmpty());

            // Verify the segments file is included
            String segmentsFileName = commit.getSegmentsFileName();
            assertTrue("Segments file should be in the file list", fileNames.contains(segmentsFileName));
        }
    }

    public void testIndexErrorHandlingPerDocument() throws IOException {
        String validSample1 = createSampleJson(series1, 1712576200L, 1024.0);
        String validSample2 = createSampleJson(series2, 1712576400L, 1026.0);
        String validSample3 = createSampleJson(series2, 1712576800L, 1026.0);

        // Create invalid sample with invalid labels that will cause RuntimeException
        String invalidLabelsJson = "{\"labels\":\"key value key2\",\"timestamp\":1712576600,\"value\":100.0}";

        // Test valid documents index successfully
        Engine.IndexResult validResult1 = publishSample(0, validSample1);
        assertTrue("Valid sample should be created", validResult1.isCreated());
        assertNull("Valid sample should not have failure", validResult1.getFailure());

        Engine.IndexResult validResult2 = publishSample(1, validSample2);
        assertTrue("Valid sample should be created", validResult2.isCreated());
        assertNull("Valid sample should not have failure", validResult2.getFailure());

        // Test invalid document returns error without crashing
        Engine.IndexResult invalidResult = publishSample(2, invalidLabelsJson);
        assertFalse("Invalid sample should not be created", invalidResult.isCreated());
        assertNotNull("Invalid sample should have failure", invalidResult.getFailure());
        assertTrue("Failure should be RuntimeException", invalidResult.getFailure() instanceof RuntimeException);

        // Verify that subsequent valid documents can still be indexed
        Engine.IndexResult validResult3 = publishSample(3, validSample3);
        assertTrue("Valid sample after error should be created", validResult3.isCreated());
        assertNull("Valid sample after error should not have failure", validResult3.getFailure());
    }

    /**
     * Test that NoOp operations are correctly handled by the engine.
     * This validates sequence number tracking and translog recording.
     */
    public void testNoOpOperation() throws IOException {
        long seqNo = 0;
        long primaryTerm = 1;

        // Create and execute a NoOp operation
        Engine.NoOp noOp = new Engine.NoOp(seqNo, primaryTerm, Engine.Operation.Origin.PRIMARY, System.nanoTime(), "test no-op");
        Engine.NoOpResult result = metricsEngine.noOp(noOp);

        // Verify result
        assertNotNull("NoOp result should not be null", result);
        assertEquals("NoOp should have correct sequence number", seqNo, result.getSeqNo());
        assertNull("NoOp should not have failure", result.getFailure());
        assertNotNull("NoOp should have translog location", result.getTranslogLocation());

        // Verify sequence number tracking
        assertEquals("Processed checkpoint should be updated", seqNo, metricsEngine.getProcessedLocalCheckpoint());
        assertEquals("Max seq no should be updated", seqNo, metricsEngine.getSeqNoStats(-1).getMaxSeqNo());
    }

    /**
     * Test that NoOp operations from translog don't get re-added to translog.
     */
    public void testNoOpFromTranslog() throws IOException {
        long seqNo = 0;
        long primaryTerm = 1;

        // Create NoOp from translog origin
        Engine.NoOp noOp = new Engine.NoOp(
            seqNo,
            primaryTerm,
            Engine.Operation.Origin.LOCAL_TRANSLOG_RECOVERY,
            System.nanoTime(),
            "translog replay"
        );
        Engine.NoOpResult result = metricsEngine.noOp(noOp);

        // Verify result
        assertNotNull("NoOp result should not be null", result);
        assertEquals("NoOp should have correct sequence number", seqNo, result.getSeqNo());
        assertNull("NoOp from translog should not have translog location", result.getTranslogLocation());

        // Verify checkpoints are updated
        assertEquals("Processed checkpoint should be updated", seqNo, metricsEngine.getProcessedLocalCheckpoint());
        assertEquals("Persisted checkpoint should be updated", seqNo, metricsEngine.getPersistedLocalCheckpoint());
    }

    public void testFillSeqNoGaps() throws IOException {
        long primaryTerm = 1;

        // Index a sample to create seq no 0
        String sample1 = createSampleJson(series1, 1000L, 100.0);
        Engine.IndexResult result1 = publishSample(0, sample1);
        assertTrue("First sample should be created", result1.isCreated());
        assertEquals("First sample should have seq no 0", 0L, result1.getSeqNo());

        // Manually advance max seq no by indexing NoOp operations to create gaps
        for (long seqNo = 5; seqNo <= 10; seqNo++) {
            Engine.NoOp noOp = new Engine.NoOp(seqNo, primaryTerm, Engine.Operation.Origin.PRIMARY, System.nanoTime(), "creating gap");
            metricsEngine.noOp(noOp);
        }

        // Verify max seq no is 5
        assertEquals("Max seq no should be 10", 10L, metricsEngine.getSeqNoStats(-1).getMaxSeqNo());

        metricsEngine.translogManager().syncTranslog();
        // fillSeqNoGaps should return 0 since there are no gaps
        int numNoOps = metricsEngine.fillSeqNoGaps(primaryTerm);
        assertEquals("Should have filled 4 gaps", 4, numNoOps);
    }

    /**
     * Test that when a series is marked as failed, subsequent requests create a new series and succeed.
     */
    public void testFailedSeriesReplacementOnRetry() throws IOException {
        // Index request
        String sample1 = createSampleJson(series1, 1000L, 100.0);
        Engine.IndexResult result1 = publishSample(0, sample1);
        assertTrue("First sample should be created", result1.isCreated());

        // Get series reference before marking as failed
        MemSeries originalSeries = metricsEngine.getHead().getSeriesMap().getByReference(series1.stableHash());
        assertNotNull("Original series should exist", originalSeries);

        // Mark series as failed - this will delete it from the seriesMap
        metricsEngine.getHead().markSeriesAsFailed(originalSeries);

        // Verify series is removed from the map
        MemSeries afterFailure = metricsEngine.getHead().getSeriesMap().getByReference(series1.stableHash());
        assertNull("Failed series should be deleted from map", afterFailure);

        // Index another request with same labels - should create a new series
        String sample2 = createSampleJson(series1, 2000L, 150.0);
        Engine.IndexResult result2 = publishSample(1, sample2);
        assertTrue("Second sample should succeed", result2.isCreated());

        // Check that a new series was created
        MemSeries newSeries = metricsEngine.getHead().getSeriesMap().getByReference(series1.stableHash());
        assertNotNull("New series should be created", newSeries);
        assertNotSame("New series should be different from original", originalSeries, newSeries);
        assertFalse("New series should not be marked as failed", newSeries.isFailed());
    }

    /**
     * Test that empty label exception is temporarily ignored and operation succeeds.
     * TODO: Delete this test once OOO support is added.
     */
    public void testEmptyLabelExceptionIgnored() throws IOException {
        // Create a document with empty labels (no labels field) but with series reference
        long seriesReference = 12345L;
        String sampleJson = "{\"reference\":" + seriesReference + ",\"timestamp\":" + 1000L + ",\"value\":" + 100.0 + "}";

        Engine.Index index = new Engine.Index(
            new Term(IdFieldMapper.NAME, Uid.encodeId("test-id")),
            new ParsedDocument(null, null, "test-id", null, null, new BytesArray(sampleJson), XContentType.JSON, null),
            0,
            1L,
            1L,
            VersionType.INTERNAL,
            Engine.Operation.Origin.PRIMARY,
            System.nanoTime(),
            -1,
            false,
            SequenceNumbers.UNASSIGNED_SEQ_NO,
            SequenceNumbers.UNASSIGNED_PRIMARY_TERM
        );

        Engine.IndexResult result = metricsEngine.index(index);

        // Empty label exception is ignored, operation succeeds
        assertTrue("Operation should succeed despite empty labels", result.isCreated());
        assertNull(result.getFailure());
        assertEquals("Seq no should be 0", 0L, result.getSeqNo());
    }

    /**
     * Test that TSDBTragicException causes the engine to throw RuntimeException.
     */
    public void testTragicExceptionHandling() throws IOException {
        // Close the engine to trigger tragic exception
        metricsEngine.close();

        String sample = createSampleJson(series1, 1000L, 100.0);
        Engine.Index index = new Engine.Index(
            new Term(IdFieldMapper.NAME, Uid.encodeId("test-id")),
            new ParsedDocument(null, null, "test-id", null, null, new BytesArray(sample), XContentType.JSON, null),
            0,
            1L,
            1L,
            VersionType.INTERNAL,
            Engine.Operation.Origin.PRIMARY,
            System.nanoTime(),
            -1,
            false,
            SequenceNumbers.UNASSIGNED_SEQ_NO,
            SequenceNumbers.UNASSIGNED_PRIMARY_TERM
        );

        // Indexing on closed engine should throw RuntimeException
        assertThrows(RuntimeException.class, () -> metricsEngine.index(index));
    }

    /**
     * Test that old segments_N files are cleaned up after flush when not snapshotted.
     */
    public void testSegmentsFileCleanupAfterFlush() throws Exception {
        // Get initial segments file name
        String initialSegmentsFile = engineStore.readLastCommittedSegmentsInfo().getSegmentsFileName();
        assertTrue("Initial segments file should exist", segmentsFileExists(initialSegmentsFile));

        // Index samples with timestamps spread across time to ensure chunks are closed
        for (int i = 0; i < 10; i++) {
            publishSample(i, createSampleJson("__name__ metric1 host server1", 100L + (i * 1000L), 10.0 + i));
        }

        // Force flush to close chunks and create new segments file
        metricsEngine.flush(true, true);

        // Get new segments file name
        String newSegmentsFile = engineStore.readLastCommittedSegmentsInfo().getSegmentsFileName();
        assertNotEquals("Segments file should be different after flush", initialSegmentsFile, newSegmentsFile);
        assertTrue("New segments file should exist", segmentsFileExists(newSegmentsFile));

        // Old segments file should be cleaned up (not snapshotted)
        assertFalse("Old segments file should be deleted after flush", segmentsFileExists(initialSegmentsFile));
    }

    /**
     * Test that snapshotted segments_N files are protected from deletion.
     */
    public void testSnapshotProtectsSegmentsFile() throws Exception {
        // Get current segments file before any operations
        String segmentsFileBeforeSnapshot = engineStore.readLastCommittedSegmentsInfo().getSegmentsFileName();

        // Index samples with timestamps spread across time
        for (int i = 0; i < 10; i++) {
            publishSample(i, createSampleJson("__name__ metric1 host server1", 1000L + (i * 1000L), 10.0 + i));
        }

        // Acquire snapshot (this should protect the segments file)
        var commitCloseable = metricsEngine.acquireSafeIndexCommit();
        try {
            String snapshotSegmentsFile = commitCloseable.get().getSegmentsFileName();
            assertEquals("Snapshot should reference current segments file", segmentsFileBeforeSnapshot, snapshotSegmentsFile);

            // Index more samples and flush to create new segments file
            for (int i = 10; i < 20; i++) {
                publishSample(i, createSampleJson("__name__ metric1 host server1", 11000L + (i * 1000L), 20.0 + i));
            }
            metricsEngine.flush(true, true);

            // Get new segments file after flush
            String newSegmentsFile = engineStore.readLastCommittedSegmentsInfo().getSegmentsFileName();
            assertNotEquals("New segments file should be created", segmentsFileBeforeSnapshot, newSegmentsFile);

            // Old segments file should still exist because it's snapshotted
            assertTrue("Snapshotted segments file should be protected from deletion", segmentsFileExists(segmentsFileBeforeSnapshot));
            assertTrue("New segments file should exist", segmentsFileExists(newSegmentsFile));

        } finally {
            // Release snapshot
            commitCloseable.close();
        }

        // After releasing snapshot, do another flush to trigger cleanup
        for (int i = 20; i < 30; i++) {
            publishSample(i, createSampleJson("__name__ metric1 host server1", 31000L + (i * 1000L), 30.0 + i));
        }
        metricsEngine.flush(true, true);

        // Now the old segments file should be cleaned up
        assertFalse("Old segments file should be deleted after snapshot release", segmentsFileExists(segmentsFileBeforeSnapshot));
    }

    /**
     * Test that metadata store commits properly manage segments_N files.
     */
    public void testMetadataStoreCommitManagesSegmentsFiles() throws Exception {
        // Get initial segments file
        String initialSegmentsFile = engineStore.readLastCommittedSegmentsInfo().getSegmentsFileName();
        assertTrue("Initial segments file should exist", segmentsFileExists(initialSegmentsFile));

        // Store metadata which triggers a commit
        metricsEngine.getMetadataStore().store("test_key", "test_value");

        // New segments file should be created
        String newSegmentsFile = engineStore.readLastCommittedSegmentsInfo().getSegmentsFileName();
        assertNotEquals("New segments file should be created after metadata store", initialSegmentsFile, newSegmentsFile);
        assertTrue("New segments file should exist", segmentsFileExists(newSegmentsFile));

        // Verify metadata is persisted
        var retrievedValue = metricsEngine.getMetadataStore().retrieve("test_key");
        assertTrue("Metadata should be retrievable", retrievedValue.isPresent());
        assertEquals("Metadata value should match", "test_value", retrievedValue.get());

        // Old segments file should be cleaned up
        assertFalse("Old segments file should be deleted", segmentsFileExists(initialSegmentsFile));
    }

    /**
     * Test that critical metadata (TRANSLOG_UUID, HISTORY_UUID) is preserved across flushes.
     */
    public void testCriticalMetadataPreservedAcrossFlushes() throws Exception {
        // Get initial metadata
        var initialSegmentInfos = engineStore.readLastCommittedSegmentsInfo();
        String initialTranslogUUID = initialSegmentInfos.getUserData().get(Translog.TRANSLOG_UUID_KEY);
        String initialHistoryUUID = initialSegmentInfos.getUserData().get(Engine.HISTORY_UUID_KEY);

        assertNotNull("Initial translog UUID should exist", initialTranslogUUID);
        assertNotNull("Initial history UUID should exist", initialHistoryUUID);

        // Index samples and flush
        for (int i = 0; i < 10; i++) {
            publishSample(i, createSampleJson("__name__ metric1 host server1", 1000L + (i * 1000L), 10.0 + i));
        }
        metricsEngine.flush(true, true);

        // Verify metadata is preserved after flush
        var segmentInfosAfterFlush = engineStore.readLastCommittedSegmentsInfo();
        String translogUUIDAfterFlush = segmentInfosAfterFlush.getUserData().get(Translog.TRANSLOG_UUID_KEY);
        String historyUUIDAfterFlush = segmentInfosAfterFlush.getUserData().get(Engine.HISTORY_UUID_KEY);

        assertEquals("Translog UUID should be preserved", initialTranslogUUID, translogUUIDAfterFlush);
        assertEquals("History UUID should be preserved", initialHistoryUUID, historyUUIDAfterFlush);
    }

    /**
     * Helper method to check if a segments file exists in the store directory.
     */
    private boolean segmentsFileExists(String segmentsFileName) {
        try {
            String[] files = engineStore.directory().listAll();
            for (String file : files) {
                if (file.equals(segmentsFileName)) {
                    return true;
                }
            }
            return false;
        } catch (IOException e) {
            throw new RuntimeException("Failed to check segments file existence", e);
        }
    }

    /**
     * Test that post_recovery refresh enables empty series dropping in flush.
     */
    public void testPostRecoveryRefreshEnablesSeriesDropping() throws Exception {
        // Index samples
        publishSample(0, createSampleJson(series1, 1000L, 100.0));
        publishSample(1, createSampleJson(series2, 9999999L, 200.0));

        assertEquals("Should have 2 series initially", 2L, metricsEngine.getHead().getNumSeries());

        // Flush without post_recovery refresh - empty series should NOT be dropped even if empty
        metricsEngine.getHead().updateMaxSeenTimestamp(9999999L);
        metricsEngine.flush(true, true);
        assertEquals("Should still have 2 series after flush without post_recovery refresh", 2L, metricsEngine.getHead().getNumSeries());

        // Call post_recovery refresh
        metricsEngine.refresh("post_recovery");
        // Now flush again - series dropping should be allowed
        metricsEngine.flush(true, true);

        assertEquals("Should have 1 series after flush with post_recovery refresh", 1L, metricsEngine.getHead().getNumSeries());
    }

    /**
     * Test that regular refresh does not enable series dropping.
     */
    public void testRegularRefreshDoesNotEnableSeriesDropping() throws Exception {
        // Index samples
        publishSample(0, createSampleJson(series1, 1000L, 100.0));

        // Call regular refresh (not post_recovery)
        metricsEngine.getHead().updateMaxSeenTimestamp(Long.MAX_VALUE);
        metricsEngine.refresh("regular_refresh");
        metricsEngine.refresh("test_refresh");
        metricsEngine.maybeRefresh("maybe_refresh");

        // Flush should still have series dropping disabled
        metricsEngine.flush(true, true);

        assertEquals("Should still have 1 series after regular refresh and flush", 1L, metricsEngine.getHead().getNumSeries());
    }

    /**
     * Test that flush handles the case when all chunks are closed.
     * When closeHeadChunks returns -1 (all chunks closed), flush should use the current processed checkpoint instead.
     */
    public void testFlushWithAllChunksClosed() throws Exception {
        // Index samples to have a processed checkpoint
        publishSample(0, createSampleJson(series1, 1000L, 100.0));
        publishSample(1, createSampleJson(series1, 2000L, 200.0));
        publishSample(2, createSampleJson(series2, 3000L, 300.0));

        long processedCheckpointBeforeFlush = metricsEngine.getProcessedLocalCheckpoint();
        assertEquals("Should have processed checkpoint of 2", 2L, processedCheckpointBeforeFlush);

        // Enable series dropping with post_recovery refresh
        metricsEngine.refresh("post_recovery");

        // Update maxTime to a very large value so all chunks become closeable
        metricsEngine.getHead().updateMaxSeenTimestamp(Long.MAX_VALUE);

        // Force flush - this should close all chunks and return -1 from closeHeadChunks
        // The flush logic should then use currentProcessedCheckpoint (2) instead of -1
        metricsEngine.flush(true, true);

        // Verify the committed checkpoint is the processed checkpoint (not -1)
        var committedSegmentInfos = engineStore.readLastCommittedSegmentsInfo();
        String committedCheckpoint = committedSegmentInfos.getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY);
        assertEquals("Committed checkpoint should be the processed checkpoint when all chunks are closed", "2", committedCheckpoint);

        // Verify persisted checkpoint matches after sync
        metricsEngine.translogManager().syncTranslog();
        assertEquals("Persisted checkpoint should be 2", 2L, metricsEngine.getPersistedLocalCheckpoint());
    }

    public void testValidateNoStubSeriesAfterRecovery() throws Exception {
        // Create a stub series by simulating recovery with ref-only operation
        Labels labels = ByteLabels.fromStrings("__name__", "metric", "host", "server1");
        long ref = labels.stableHash();

        assertEquals("Stub counter should start at 0", 0L, metricsEngine.getHead().getSeriesMap().getStubSeriesCount());
        metricsEngine.getHead().getOrCreateSeries(ref, null, 1000L);

        MemSeries stubSeries = metricsEngine.getHead().getSeriesMap().getByReference(ref);
        assertNotNull("Stub series should exist", stubSeries);
        assertTrue("Should be stub", stubSeries.isStub());
        assertEquals("Stub counter should be 1 after creating stub", 1L, metricsEngine.getHead().getSeriesMap().getStubSeriesCount());
    }

    public void testNoStubSeriesAfterProperRecovery() throws Exception {
        // Create and upgrade a stub series properly
        Labels labels = ByteLabels.fromStrings("__name__", "metric", "host", "server1");
        long ref = labels.stableHash();

        assertEquals("Stub counter should start at 0", 0L, metricsEngine.getHead().getSeriesMap().getStubSeriesCount());

        // Create stub
        metricsEngine.getHead().getOrCreateSeries(ref, null, 1000L);
        assertTrue("Should be stub", metricsEngine.getHead().getSeriesMap().getByReference(ref).isStub());
        assertEquals("Stub counter should be 1 after creating stub", 1L, metricsEngine.getHead().getSeriesMap().getStubSeriesCount());

        // Upgrade with labels
        metricsEngine.getHead().getOrCreateSeries(ref, labels, 2000L);

        assertFalse("Should be upgraded", metricsEngine.getHead().getSeriesMap().getByReference(ref).isStub());
        assertEquals("Stub counter should be 0 after upgrade", 0L, metricsEngine.getHead().getSeriesMap().getStubSeriesCount());
    }

    /**
     * Test that flush is throttled by commit_interval
     */
    public void testFlushThrottledByCommitInterval() throws Exception {
        TimeValue commitInterval = TimeValue.timeValueSeconds(10);

        // Close existing engine and create new one with long commit interval
        metricsEngine.close();
        engineStore.close();

        IndexSettings settingsWithInterval = IndexSettingsModule.newIndexSettings(
            "index",
            Settings.builder()
                .put("index.tsdb_engine.enabled", true)
                .put("index.queries.cache.enabled", false)
                .put("index.requests.cache.enable", false)
                .put("index.tsdb_engine.commit_interval", commitInterval.getStringRep())
                .put("index.translog.flush_threshold_size", "56b") // low threshold so translog size check passes
                .build()
        );

        MutableClock mutableClock = new MutableClock(0L);

        // Reset engineConfig and rebuild engine
        engineConfig = null;
        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);
        engineStore = createStore(settingsWithInterval, newDirectory());
        metricsEngine = buildTSDBEngine(globalCheckpoint, engineStore, settingsWithInterval, clusterApplierService, mutableClock);
        metricsEngine.translogManager()
            .recoverFromTranslog(this.translogHandler, metricsEngine.getProcessedLocalCheckpoint(), Long.MAX_VALUE);

        // Index some data
        String sample1 = createSampleJson("__name__ test_metric host server1", 1000, 10.0);
        publishSample(0, sample1);

        long generationBeforeFirstFlush = engineStore.readLastCommittedSegmentsInfo().getGeneration();

        // First flush with force=true - bypasses translog size check forcing a flush
        metricsEngine.flush(true, true);

        // Get generation after first flush
        long generationAfterFirstFlush = engineStore.readLastCommittedSegmentsInfo().getGeneration();
        assertTrue("Generation should increase after first flush", generationAfterFirstFlush > generationBeforeFirstFlush);

        // Index more data
        String sample2 = createSampleJson("__name__ test_metric host server2", 2000, 20.0);
        publishSample(1, sample2);

        // Advance clock, but not enough to make flush eligible to commit
        mutableClock.advance(commitInterval.millis() / 2);

        // Immediate second flush with force=false - should be throttled by commit_interval (no new commit)
        // waitIfOngoing = true to simulate a flush triggered due to translog size
        metricsEngine.flush(false, true);

        // Generation should be unchanged (flush was throttled by commit_interval)
        long generationAfterThrottledFlush = engineStore.readLastCommittedSegmentsInfo().getGeneration();
        assertEquals(
            "Generation should not change when flush is throttled by commit_interval",
            generationAfterFirstFlush,
            generationAfterThrottledFlush
        );

        // Advance clock to make flush eligible to commit
        mutableClock.advance(commitInterval.millis());

        // Third flush with force=true - should succeed now that interval has elapsed
        metricsEngine.flush(false, true);

        // Generation should increase (proving rate limiter was the blocker)
        long generationAfterIntervalElapsed = engineStore.readLastCommittedSegmentsInfo().getGeneration();
        assertTrue(
            "Generation should increase after commit interval elapses (proves rate limiter was blocking)",
            generationAfterIntervalElapsed > generationAfterThrottledFlush
        );

        // Verify data is still accessible
        metricsEngine.refresh("test");
        assertEquals("Should have 2 series", 2L, metricsEngine.getHead().getNumSeries());
    }

    /**
     * Test that flush is throttled by commit_interval
     */
    public void testForceFlushNotThrottledByCommitInterval() throws Exception {
        TimeValue commitInterval = TimeValue.timeValueSeconds(10);

        // Close existing engine and create new one with long commit interval
        metricsEngine.close();
        engineStore.close();

        IndexSettings settingsWithInterval = IndexSettingsModule.newIndexSettings(
            "index",
            Settings.builder()
                .put("index.tsdb_engine.enabled", true)
                .put("index.queries.cache.enabled", false)
                .put("index.requests.cache.enable", false)
                .put("index.tsdb_engine.commit_interval", commitInterval.getStringRep())
                .put("index.translog.flush_threshold_size", "56b") // low threshold so translog size check passes
                .build()
        );

        MutableClock mutableClock = new MutableClock(0L);

        // Reset engineConfig and rebuild engine
        engineConfig = null;
        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);
        engineStore = createStore(settingsWithInterval, newDirectory());
        metricsEngine = buildTSDBEngine(globalCheckpoint, engineStore, settingsWithInterval, clusterApplierService, mutableClock);
        metricsEngine.translogManager()
            .recoverFromTranslog(this.translogHandler, metricsEngine.getProcessedLocalCheckpoint(), Long.MAX_VALUE);

        // Index some data
        String sample1 = createSampleJson("__name__ test_metric host server1", 1000, 10.0);
        publishSample(0, sample1);

        long generationBeforeFirstFlush = engineStore.readLastCommittedSegmentsInfo().getGeneration();

        // First flush with force=true - bypasses translog size check forcing a flush
        metricsEngine.flush(true, true);

        // Get generation after first flush
        long generationAfterFirstFlush = engineStore.readLastCommittedSegmentsInfo().getGeneration();
        assertTrue("Generation should increase after first flush", generationAfterFirstFlush > generationBeforeFirstFlush);

        // Index more data
        String sample2 = createSampleJson("__name__ test_metric host server2", 2000, 20.0);
        publishSample(1, sample2);

        // Advance clock, but not enough to make flush eligible to commit
        mutableClock.advance(commitInterval.millis() / 2);

        // Test force flush, when non-forced flushes are ineligible to commit
        metricsEngine.flush(true, true);

        // Generation should increase (proving rate limiter was the blocker)
        long generationAfterForceFlush = engineStore.readLastCommittedSegmentsInfo().getGeneration();
        assertTrue(
            "Generation should increase after commit interval elapses (proves rate limiter was blocking)",
            generationAfterForceFlush > generationAfterFirstFlush
        );

        // Verify data is still accessible
        metricsEngine.refresh("test");
        assertEquals("Should have 2 series", 2L, metricsEngine.getHead().getNumSeries());
    }
}
